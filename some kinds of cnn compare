



LeNet5
最早的卷积神经网络之一，诞生于1994，有Yann LeCun完成并命名。
论文《Gradient-Based Learning Applied to Document Recognition》

LeNet5架构观点：
  1.图像的特征分布在整张图像上，卷积核作为可学习的参数提取多个位置的相似特征。
  2.像素点不应该作为第一层输入，因为图像有很强的空间相关性，作为第一层输入可能利用不到这些相关性

LeNet5架构描述：
  source -> [ conv ->pool -> nolinear ] *n -> fc*n -> output

LeNet5总结：
  1.卷积神经网络使用三种层为一次处理：圈基层，池化层，非线性层
  2.圈基层提取图片的空间特征
  3.使用映射带空间均值下的采样（subsample）
  4.非线性的激活函数tanh, sigmoid激活
  5.多层神经网络（mlp）分类器
  6.层与层之间用系数举证避免大的计算成本（这里我的理解就是卷积核什么的在同层是公用的，减少了巨量参数）

结语：LeNet5是大量神经网络架构的起点，给这个领域带来了灵感。膜拜大佬一次。


Dan Ciresan Net
2010年，由Dan Cludiu Ciresan 和Jurgen Schmidhuber发布的最高的GPU神经网络，运行于GTX280,9层
包含前向与反向传播
“未了解完全，等待更新中”



AlexNet
2012年由Alex Krizhevsky提出的深度卷积神经网络模型，可以看作是LeNet5的更深更宽的一种版本

技术新点：
  Relu, Dropout, LRN等
 
结构简介：
  包含6亿3000万个连接，6000万个参数和65万个神经元
  拥有五个卷积层，其中三个卷积层后面有最大池化层，最后还有三个全连接层
  （感觉好像和LeNet5网络差不多了，只是深度变得更深了，还有就是上面说的那几种技术吧，特别是Dropout,亲测效果很好）

新技术详解：
  1.ReLU作为CNN的激活函数，在较深的网络中已验证超过了Sigmoid，
    成功解决了Sigmoid在网络较深时候的梯度弥散问题，也算是ReLU的扬眉吐气吧
  
  2.Dropout：训练时按照一定比例忽略一部分神经元，避免模型出现过拟合
    而且其使用范围仅限于最后的fc全连接层，并不影响前面卷积和池化提取图像的空间特征
   
  3.max_pool:此前CNN中普遍使用平均池化，Alex中使用了最大池化，避免了平均池化的模糊化效果
    而且Alex中提出让步长比池化核的尺寸小，这样池化曾的输出之间有重叠和讣告，丰富了特征
   
  4.lrn(局部响应归一化)：模拟活体神经元，对局部神经元的活动创建竞争机制
    使得其中相应比较大的值变得相对更大，并一直其他反馈较小的神经元，提高模型的泛化能力。
    
  5.CUDA加速，两块CPU同时运行，可以同时访问显存，并且只在网络某些层进行通信，控制通信的性能损耗。
  
  6.数据增强
  
AlexNet架构：
  8个需要训练参数的层（不包括池化层和LRN层），前五层为卷积，后3层为全连接层，
  最后一层是1000类输出的softmax层作为分类
  LRN出现在第一个及第二个卷积层后，最大池化层出现在两个LRN层和最后一个卷积层后
  ReLU激活函数则应用在这8层每一层的后面。
  

  
  Overfeat
    2013年，纽约大学Yann LeCun实验室提出AlexNet的衍生--Overfeat
    (参见：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks)
    这篇文章也提出了学习边界框（learning bounding box）。
   
   
 VGG
  牛津大学VGG网络（参见：Very Deep Convolutional Networks for Large-Scale Image Recognition）
  是第一个在各个卷积层使用更小的3*3卷积核，并把他们组合作为卷积序列进行处理的网络
  
  简介：
    其卷积的原理卡尼起来与LeNet5原理相反，大的卷积核作为获取一张图像中相似的特征，
    与AlexNet大卷积核不同，VGG网络的卷积核变得很小，离LeNet5竭力避免的1*1卷积接近
    ----至少在第一层就是。
    VGG通过一次采用多个3*3卷积，能够模仿出更大的感受野效果
